{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer-Learning_InceptionV3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtYRbeeNsRzT7GCWZ79aoo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasheymateen/Transfer-Learning_Convolutional_Network/blob/master/Transfer_Learning_InceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrADxx5bkiB_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQzAflhn32V7",
        "outputId": "e0f5ea5e-e52f-4aa9-9786-4d2ff454854e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\" Transfer Learning \"\"\"\n",
        "\n",
        "import tensorflow.keras as K\n",
        "\n",
        "\n",
        "def preprocess_data(X, Y):\n",
        "    \"\"\" making the preprocess of data \"\"\"\n",
        "    X_final = K.applications.inception_v3.preprocess_input(X)\n",
        "    Y_final = K.utils.to_categorical(Y, 10)\n",
        "    return X_final, Y_final\n",
        "\n",
        "\n",
        "def model():\n",
        "    \"\"\" model to classify CIFAR 10 \"\"\"\n",
        "    inception = K.applications.InceptionV3(include_top=False,\n",
        "                                           input_shape=(128, 128, 3))\n",
        "    inception.layers.pop()\n",
        "    model = K.Sequential()\n",
        "    model.add(K.layers.UpSampling2D(size=(4, 4)))\n",
        "    model.add(inception)\n",
        "    model.add(K.layers.Flatten())\n",
        "    model.add(K.layers.Dense(units=128, activation='relu',\n",
        "                             kernel_initializer='he_normal'))\n",
        "    model.add(K.layers.Dense(units=10, activation='softmax',\n",
        "                             kernel_initializer='he_normal'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(network, data, labels, batch_size, epochs,\n",
        "                validation_data=None, early_stopping=False,\n",
        "                patience=0, alpha=0.1, decay_rate=1,\n",
        "                filepath=None, verbose=True, shuffle=False):\n",
        "    \"\"\"\n",
        "    trains model using mini-batch graident descent\n",
        "    network - tf.Tensor - model to train\n",
        "    data - numpy.ndarray - contains input data\n",
        "    labels - numpy.ndarray - onehot array w/labels of data\n",
        "    batch_size - size of mini batch in int\n",
        "    epochs - number of epochs in int\n",
        "    verbose - output should be printed or not determined by bool\n",
        "    shuffle - whether to shuffle the batches, determined by bool\n",
        "    validationdata - tupel data to validate the model\n",
        "    early_stopping - should early stopping be used, det. by bool\n",
        "    patience - patience for early stopping, det by bool\n",
        "    learning_ratedecay - should learning rate decay be used\n",
        "    alpha - initial learning rate float\n",
        "    decay_rate - float\n",
        "    save_best - either to save the model after epoch or not\n",
        "    filepath - path of saved model\n",
        "    Return: tf.History - object result generated by training the model\n",
        "    \"\"\"\n",
        "\n",
        "    def learning_rate_decay(epoch):\n",
        "        \"\"\" learning rate callback \"\"\"\n",
        "        alpha_utd = alpha / (1 + (decay_rate * epoch))\n",
        "        return alpha_utd\n",
        "\n",
        "    callbacks = []\n",
        "    checkpoint = K.callbacks.ModelCheckpoint(filepath=filepath,\n",
        "                                             save_best_only=True,\n",
        "                                             monitor='val_loss',\n",
        "                                             mode='min')\n",
        "    callbacks.append(checkpoint)\n",
        "\n",
        "    if validation_data:\n",
        "        decay = K.callbacks.LearningRateScheduler(learning_rate_decay,\n",
        "                                                  verbose=1)\n",
        "        callbacks.append(decay)\n",
        "    if validation_data and early_stopping:\n",
        "        EarlyStopping = K.callbacks.EarlyStopping(patience=patience,\n",
        "                                                  monitor='val_loss',\n",
        "                                                  mode='min')\n",
        "        callbacks.append(EarlyStopping)\n",
        "    return network.fit(data, labels, batch_size=batch_size,\n",
        "                       epochs=epochs, verbose=verbose,\n",
        "                       shuffle=shuffle,\n",
        "                       validation_data=validation_data,\n",
        "                       callbacks=callbacks)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    (X_train, Y_train), (X_valid, Y_valid) = K.datasets.cifar10.load_data()\n",
        "    X_train, Y_train = preprocess_data(X_train, Y_train)\n",
        "    X_valid, Y_valid = preprocess_data(X_valid, Y_valid)\n",
        "    model = model()\n",
        "    train_model(model, X_train, Y_train, 64, 30,\n",
        "                validation_data=(X_valid, Y_valid), early_stopping=True,\n",
        "                patience=3, alpha=0.001, filepath='cifar10.h5')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "Epoch 1/30\n",
            "782/782 [==============================] - 120s 154ms/step - loss: 1.1956 - accuracy: 0.5866 - val_loss: 0.8274 - val_accuracy: 0.7155\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0005.\n",
            "Epoch 2/30\n",
            "782/782 [==============================] - 118s 150ms/step - loss: 0.6822 - accuracy: 0.7675 - val_loss: 1.1287 - val_accuracy: 0.6351\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0003333333333333333.\n",
            "Epoch 3/30\n",
            "782/782 [==============================] - 119s 152ms/step - loss: 0.4651 - accuracy: 0.8458 - val_loss: 0.5581 - val_accuracy: 0.8128\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00025.\n",
            "Epoch 4/30\n",
            "782/782 [==============================] - 119s 152ms/step - loss: 0.2679 - accuracy: 0.9112 - val_loss: 0.5211 - val_accuracy: 0.8453\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0002.\n",
            "Epoch 5/30\n",
            "782/782 [==============================] - 118s 151ms/step - loss: 0.1413 - accuracy: 0.9540 - val_loss: 0.5720 - val_accuracy: 0.8574\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00016666666666666666.\n",
            "Epoch 6/30\n",
            "782/782 [==============================] - 118s 151ms/step - loss: 0.0804 - accuracy: 0.9736 - val_loss: 0.6445 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00014285714285714287.\n",
            "Epoch 7/30\n",
            "782/782 [==============================] - 118s 151ms/step - loss: 0.0425 - accuracy: 0.9863 - val_loss: 0.6563 - val_accuracy: 0.8674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UBnHP5T33K_"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import tensorflow.keras as K\n",
        "preprocess_data = __import__('0-transfer').preprocess_data\n",
        "\n",
        "# to fix issue with saving keras applications\n",
        "K.learning_phase = K.backend.learning_phase\n",
        "\n",
        "_, (X, Y) = K.datasets.cifar10.load_data()\n",
        "X_p, Y_p = preprocess_data(X, Y)\n",
        "model = K.models.load_model('cifar10.h5')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqoU_n7em3Qj",
        "outputId": "2e5e6fe0-98dc-4590-b6fc-0747c684c449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}